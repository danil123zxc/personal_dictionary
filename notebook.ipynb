{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:43:22.251760Z",
     "iopub.status.busy": "2025-08-06T21:43:22.251327Z",
     "iopub.status.idle": "2025-08-06T21:43:29.773814Z",
     "shell.execute_reply": "2025-08-06T21:43:29.772479Z",
     "shell.execute_reply.started": "2025-08-06T21:43:22.251717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "!pip install langchain>=0.2.0, langchain-ollama>=0.1.0, pydantic>=2.0, typing-extensions>=4.9.0, langchain-community>=0.3.27, langgraph>=0.3.66, chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:43:29.776758Z",
     "iopub.status.busy": "2025-08-06T21:43:29.776363Z",
     "iopub.status.idle": "2025-08-06T21:43:34.703058Z",
     "shell.execute_reply": "2025-08-06T21:43:34.701150Z",
     "shell.execute_reply.started": "2025-08-06T21:43:29.776728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U jq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:43:34.705341Z",
     "iopub.status.busy": "2025-08-06T21:43:34.704892Z",
     "iopub.status.idle": "2025-08-06T21:43:34.716918Z",
     "shell.execute_reply": "2025-08-06T21:43:34.715624Z",
     "shell.execute_reply.started": "2025-08-06T21:43:34.705289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import List, Optional, Dict, Tuple, Any, Union, Set\n",
    "import regex\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from typing import TypedDict, List, Dict, Optional, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.schema import Document \n",
    "from langchain_core.messages import BaseMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:45:37.796787Z",
     "iopub.status.busy": "2025-08-06T21:45:37.796401Z",
     "iopub.status.idle": "2025-08-06T21:46:19.932679Z",
     "shell.execute_reply": "2025-08-06T21:46:19.930549Z",
     "shell.execute_reply.started": "2025-08-06T21:45:37.796760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# let's download and host/open ollama server on kaggle\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "subprocess.Popen(\"ollama serve\", shell=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:19.939327Z",
     "iopub.status.busy": "2025-08-06T21:46:19.937886Z",
     "iopub.status.idle": "2025-08-06T21:46:19.949625Z",
     "shell.execute_reply": "2025-08-06T21:46:19.948489Z",
     "shell.execute_reply.started": "2025-08-06T21:46:19.939268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# now we need to download the model we want to use, here it is gemma3n latest\n",
    "subprocess.Popen(\"ollama pull gemma3n\", shell=True)\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:19.950894Z",
     "iopub.status.busy": "2025-08-06T21:46:19.950507Z",
     "iopub.status.idle": "2025-08-06T21:46:24.953833Z",
     "shell.execute_reply": "2025-08-06T21:46:24.952250Z",
     "shell.execute_reply.started": "2025-08-06T21:46:19.950860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating our model with Ollama\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3n\",\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions that generate outputs with gemma3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:32.728189Z",
     "iopub.status.busy": "2025-08-06T21:46:32.727654Z",
     "iopub.status.idle": "2025-08-06T21:46:32.736711Z",
     "shell.execute_reply": "2025-08-06T21:46:32.735112Z",
     "shell.execute_reply.started": "2025-08-06T21:46:32.728154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Laguages to codes and codes to languages dictionaries\n",
    "language_codes = {\n",
    "    \"English\": \"en\",\n",
    "    \"Русский\": \"ru\",\n",
    "    \"한국어\": \"ko\",\n",
    "    \"中文\": \"zh\",\n",
    "    \"日本語\": \"ja\",\n",
    "    \"Español\": \"es\",\n",
    "    \"Français\": \"fr\",\n",
    "    \"Deutsch\": \"de\",\n",
    "    \"Italiano\": \"it\",\n",
    "    \"Português\": \"pt\"\n",
    "}\n",
    "codes_language = {\n",
    "    \"en\":\"English\",\n",
    "    \"ru\":\"Русский\",\n",
    "    \"ko\":\"한국어\",\n",
    "    \"zh\":\"中文\",\n",
    "    \"ja\":\"日本語\",\n",
    "    \"es\":\"Español\",\n",
    "    \"fr\":\"Français\",\n",
    "    \"de\":\"Deutsch\",\n",
    "    \"it\":\"Italiano\",\n",
    "    \"pt\":\"Português\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:37.755076Z",
     "iopub.status.busy": "2025-08-06T21:46:37.754667Z",
     "iopub.status.idle": "2025-08-06T21:46:37.789356Z",
     "shell.execute_reply": "2025-08-06T21:46:37.787729Z",
     "shell.execute_reply.started": "2025-08-06T21:46:37.755048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationResponse(RootModel[Dict[str, List[str]]]):\n",
    "    \"Dictionary where keys are words (lemmas) and values are translations\"\n",
    "    pass\n",
    " \n",
    "class DefinitionResponse(BaseModel):\n",
    "    definition: List[str] = Field(description=\"List of definition in the target language\")\n",
    "\n",
    "class ExamplesResponse(BaseModel):\n",
    "    examples: List[str] = Field(description=\"List of usage examples in the target language\")\n",
    "\n",
    "def generate_translation(text: str, src_language: str, tgt_language: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate translations in a given text from the source language to the target language.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing one or more sentences to translate.\n",
    "        src_language (str): The language of the original text (source language).\n",
    "        tgt_language (str): The language into which the text will be translated (target language).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            {{\n",
    "                'word1': \n",
    "                {   \n",
    "                    'translations': [''],\n",
    "                },\n",
    "                'word2': \n",
    "                {   \n",
    "                    'translations': [''],\n",
    "                }\n",
    "            }}\n",
    " \n",
    "    \"\"\"\n",
    "    # Create a translation prompt using PromptTemplate\n",
    "    translation = PromptTemplate(\n",
    "        input_variables=[\"text\", \"src_language\", \"tgt_language\"],\n",
    "        template=translation_prompt\n",
    "    )\n",
    "\n",
    "    # Format the prompt with provided variables\n",
    "    prompt = translation.format(\n",
    "        text=text,\n",
    "        src_language=src_language,\n",
    "        tgt_language=tgt_language,\n",
    "    )\n",
    "\n",
    "    # Use the LLM to generate the translation\n",
    "    response = llm.invoke(prompt, config={\"response_format\": \"json_object\"})\n",
    "\n",
    "    # Create parser instance with the response model\n",
    "    parser = JsonOutputParser(pydantic_object=TranslationResponse)\n",
    "    \n",
    "    try:\n",
    "        # Parse the response into the defined structure\n",
    "        parsed_response = parser.parse(response.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        parsed_response = {\n",
    "            \"original\": text,\n",
    "            \"message\": f\"Error parsing response: {e}\"\n",
    "        }\n",
    "\n",
    "    return parsed_response\n",
    "    \n",
    "def generate_definition(word: str, language: str,  context: Optional[str]=None) -> dict:\n",
    "    \"\"\"\n",
    "    Generate definition for a given word in a specified language.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to define.\n",
    "        language (str): The language in which definitions are to be provided.\n",
    "        context (str): The original sentence or context in which the word is used.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "        {{\n",
    "            \"definition\": [\"single definition\"],\n",
    "        }}\n",
    "    \"\"\"\n",
    "    # Create a definition prompt using PromptTemplate\n",
    "    definition = PromptTemplate(\n",
    "        input_variables=[\"word\", \"language\", \"context\"],\n",
    "        template=definition_prompt  # Changed from example_prompt to definition_prompt\n",
    "    )\n",
    "    \n",
    "    # Format the prompt with the provided variables\n",
    "    prompt = definition.format(\n",
    "        language=language,\n",
    "        word=word,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    # Use the llm to invoke the prompt and get the response\n",
    "    response = llm.invoke(prompt, config={\"response_format\": \"json_object\"})\n",
    "\n",
    "    # Create parser instance with the response model\n",
    "    parser = JsonOutputParser(pydantic_object=DefinitionResponse)\n",
    "    \n",
    "    try:\n",
    "        # Parse the response into the defined structure\n",
    "        parsed_response = parser.parse(response.content)\n",
    "        return parsed_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"original\": word,\n",
    "            \"message\": f\"Failed to parse response: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def generate_examples(word: str, language: str, definition: Optional[str] = None, examples_number: Optional[int] = 1) -> dict:\n",
    "    \"\"\"\n",
    "    Generate examples of usage for a given type in a foreign language.\n",
    "    Args:\n",
    "        word(str)\n",
    "        language (str): The language in which examples are to be provided.  \n",
    "        definition (str): The definition of the word or phrase.\n",
    "        examples_number (int): The number of examples to generate.\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "        {\n",
    "            \"examples\": [\"example sentence 1\", \"example sentence 2\", ...],\n",
    "            \"examples_number\": {examples_number},\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Create an example prompt using PromptTemplate\n",
    "    example = PromptTemplate(\n",
    "        input_variables=[\"word\", \"foreign_language\", \"definition\", \"examples_number\"],\n",
    "        template=example_prompt\n",
    "    )\n",
    "    # Format the prompt with the provided variables\n",
    "    prompt = example.format(word=word, \n",
    "                            language=language, \n",
    "                            definition=definition, \n",
    "                            examples_number=examples_number)\n",
    "    # Use the llm to invoke the prompt and get the response\n",
    "    response = llm.invoke(prompt, config={\"response_format\": \"json_object\"})\n",
    "\n",
    "    # Create parser instance with the response model\n",
    "    parser = JsonOutputParser(pydantic_object=ExamplesResponse)\n",
    "    \n",
    "    try:\n",
    "        # Parse the response into the defined structure\n",
    "        parsed_response = parser.parse(response.content)\n",
    "        return parsed_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"original\": word,\n",
    "            \"message\": \"Failed to parse response\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:38.087590Z",
     "iopub.status.busy": "2025-08-06T21:46:38.086503Z",
     "iopub.status.idle": "2025-08-06T21:46:38.098836Z",
     "shell.execute_reply": "2025-08-06T21:46:38.097175Z",
     "shell.execute_reply.started": "2025-08-06T21:46:38.087542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "translation_prompt = translation_prompt = \"\"\"\n",
    "You receive a text.\n",
    "\n",
    "Task:\n",
    "1. For each sentence, extract all **content words** only (exclude stopwords such as articles, prepositions, conjunctions, auxiliaries, pronouns, names).\n",
    "2. Convert each extracted word to its **base dictionary form (lemma)** — for example: \"told\" → \"tell\", \"running\" → \"run\".\n",
    "3. Translate each lemma from {src_language} to **{tgt_language}** (use the target language exactly as provided, not English by default).\n",
    "4. Remove duplicates — only include unique lemmas for the whole input text.\n",
    "5. Output as a JSON object in the format:\n",
    "    {{\n",
    "        'word1': 'translation',\n",
    "        'word2': 'translation'\n",
    "    }}\n",
    "Only include unique words per input (no duplicates).\n",
    "Do not add extra explanations or context.\n",
    "\n",
    "Input text:\n",
    "\"{text}\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "definition_prompt = \"\"\"\n",
    "    Generate a single definition for the word \"{word}\" in {language}, based only on its meaning in the given sentence:\n",
    "\n",
    "    Sentence:\n",
    "    \"{context}\"\n",
    "\n",
    "    Rules:\n",
    "    1. Provide exactly one short, clear definition matching the word's meaning in this sentence.\n",
    "    2. No other meanings, no examples.\n",
    "    3. The definition must be in {language}.\n",
    "    4. If unsure, mark as such and give your best guess.\n",
    "\n",
    "    Return JSON only in the format:\n",
    "    {{\n",
    "        \"definition\": \"single definition\",\n",
    "    }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "example_prompt = \"\"\"\n",
    "    Generate {examples_number} simple sentences for \"{word}\" in {language} using only the given definition:\n",
    "\n",
    "    Definition:\n",
    "    \"{definition}\"\n",
    "\n",
    "    Rules:\n",
    "    1. Sentences must match this definition exactly; ignore other meanings.\n",
    "    2. Grammar must be correct and natural.\n",
    "    3. Use simple, clear vocabulary for learners.\n",
    "    4. Avoid repeating the same structure or idea.\n",
    "    5. \"message\" field is only for missing/uncertain examples or rare meanings.\n",
    "\n",
    "    Output JSON only:\n",
    "\n",
    "    {{\n",
    "        \"examples\": [\"example sentence 1\", ...],\n",
    "    }}\n",
    "\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:38.382047Z",
     "iopub.status.busy": "2025-08-06T21:46:38.380765Z",
     "iopub.status.idle": "2025-08-06T21:46:38.394061Z",
     "shell.execute_reply": "2025-08-06T21:46:38.392389Z",
     "shell.execute_reply.started": "2025-08-06T21:46:38.382001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_dict_to_json(\n",
    "    dictionary: Dict[Union[str, Tuple[str, str]], Any],\n",
    "    file_path: str = \"dictionary.json\"\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Save a dictionary to a JSON file.\n",
    "\n",
    "    This function serializes the provided dictionary into JSON format and saves it to a file. \n",
    "    If the dictionary contains tuple keys, they will be automatically converted to strings \n",
    "    because JSON does not support tuple keys.\n",
    "\n",
    "    Args:\n",
    "        dictionary (Dict[Union[str, Tuple[str, str]], Any]):\n",
    "            The dictionary to save. Keys can be strings or tuples of strings.\n",
    "        file_path (str, optional):\n",
    "            The path to the JSON file where the dictionary should be saved.\n",
    "            Defaults to \"translations.json\".\n",
    "\n",
    "    Returns:\n",
    "        bool:\n",
    "            True if the dictionary was saved successfully, False if an error occurred.\n",
    "\n",
    "    Notes:\n",
    "        - Uses UTF-8 encoding for proper Unicode support.\n",
    "        - Sets `ensure_ascii=False` to preserve non-ASCII characters in readable form.\n",
    "        - Uses `indent=4` for pretty printing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert tuple keys to strings to make JSON serializable\n",
    "        serializable_dict = {\n",
    "            str(k) if isinstance(k, tuple) else k: v\n",
    "            for k, v in dictionary.items()\n",
    "        }\n",
    "\n",
    "        # Write dictionary to JSON file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(serializable_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"[ERROR] Failed to serialize dictionary to JSON: {e}\")\n",
    "        return False\n",
    "    except OSError as e:\n",
    "        print(f\"[ERROR] Failed to write to file '{file_path}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:38.532108Z",
     "iopub.status.busy": "2025-08-06T21:46:38.531745Z",
     "iopub.status.idle": "2025-08-06T21:46:38.539139Z",
     "shell.execute_reply": "2025-08-06T21:46:38.537821Z",
     "shell.execute_reply.started": "2025-08-06T21:46:38.532073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_en = '''Yesterday, I went to the park with my friend. The sun was shining and the birds were singing.\n",
    "We walked along the river and saw some ducks.\n",
    "After that, we sat on a bench and ate ice cream.\n",
    "It was a perfect day.\n",
    "'''\n",
    "text_ru = \"\"\"Вчера я ходил в парк с другом. Солнце светило, а птицы пели.\n",
    "Мы гуляли вдоль реки и видели уток.\n",
    "Потом мы сели на скамейку и ели мороженое.\n",
    "Это был прекрасный день.\"\"\"\n",
    "text_kr = \"\"\"어제 저는 친구와 함께 공원에 갔어요. 해가 빛나고 새들이 노래했어요.\n",
    "우리는 강을 따라 걸으면서 오리를 봤어요.\n",
    "그 다음 벤치에 앉아서 아이스크림을 먹었어요.\n",
    "정말 완벽한 하루였어요.\"\"\"\n",
    "text_jp = \"\"\"昨日、私は友達と一緒に公園へ行きました。太陽が輝き、鳥が歌っていました。\n",
    "川沿いを歩きながら、カモを見ました。\n",
    "そのあと、ベンチに座ってアイスクリームを食べました。\n",
    "とても素敵な一日でした。\"\"\"\n",
    "text_sp = \"\"\"Ayer fui al parque con mi amigo. El sol brillaba y los pájaros cantaban.\n",
    "Caminamos junto al río y vimos algunos patos.\n",
    "Después nos sentamos en un banco y comimos helado.\n",
    "Fue un día perfecto.\"\"\"\n",
    "text_fr = \"\"\"Hier, je suis allé au parc avec mon ami. Le soleil brillait et les oiseaux chantaient.\n",
    "Nous avons marché le long de la rivière et vu des canards.\n",
    "Ensuite, nous nous sommes assis sur un banc et avons mangé une glace.\n",
    "C’était une journée parfaite.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:38.829932Z",
     "iopub.status.busy": "2025-08-06T21:46:38.828814Z",
     "iopub.status.idle": "2025-08-06T21:46:39.032439Z",
     "shell.execute_reply": "2025-08-06T21:46:39.030046Z",
     "shell.execute_reply.started": "2025-08-06T21:46:38.829872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:39.035839Z",
     "iopub.status.busy": "2025-08-06T21:46:39.035452Z",
     "iopub.status.idle": "2025-08-06T21:46:40.076438Z",
     "shell.execute_reply": "2025-08-06T21:46:40.075125Z",
     "shell.execute_reply.started": "2025-08-06T21:46:39.035805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#loading our dictionary\n",
    "file_path = \"/kaggle/working/dictionary.json\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            dictionary_data = json.load(f)\n",
    "        print(f\"[INFO] Dictionary loaded: {len(dictionary_data)} words.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[ERROR] File {file_path} is corrupted or not valid JSON.\")\n",
    "        dictionary_data = {}\n",
    "else:\n",
    "    print(f\"[WARNING] File {file_path} not found. Using an empty dictionary.\")\n",
    "    dictionary_data = {}\n",
    "    \n",
    "docs = [Document(page_content=word) for word in dictionary_data.keys()]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name =  \"all-MiniLM-L6-v2\")\n",
    "db  = Chroma.from_documents(docs , embeddings , persist_directory='/kaggle/working/retriever')\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:40.078896Z",
     "iopub.status.busy": "2025-08-06T21:46:40.078457Z",
     "iopub.status.idle": "2025-08-06T21:46:40.086308Z",
     "shell.execute_reply": "2025-08-06T21:46:40.085143Z",
     "shell.execute_reply.started": "2025-08-06T21:46:40.078862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_similar_words_rag(word: str, text: str, k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve a list of words similar to the given input word using a RAG (Retrieval-Augmented Generation) retriever.\n",
    "\n",
    "    This function queries a retriever object for documents related to the given `word` \n",
    "    and then extracts similar words from the retrieved content. The retrieved documents \n",
    "    are processed as either JSON (from which keys are extracted) or plain text.\n",
    "\n",
    "    Args:\n",
    "        word (str):\n",
    "            The input word to search for.\n",
    "        text (str):\n",
    "            The reference text or corpus to be searched against.\n",
    "            (Currently unused in this implementation.)\n",
    "        k (int, optional):\n",
    "            The maximum number of retrieved results to process.\n",
    "            Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]:\n",
    "            A list of similar words found in the retrieved documents.\n",
    "            Returns an empty list if retrieval fails or no valid data is found.\n",
    "\n",
    "    Notes:\n",
    "        - Requires a global `retriever` object with an `.invoke()` method \n",
    "          that returns either a list of documents or a single document.\n",
    "        - Each document must have a `.page_content` attribute.\n",
    "        - If `.page_content` is JSON, the keys are extracted as similar words.\n",
    "          If it's plain text, the content itself is added to the result.\n",
    "        - All retrieval and parsing errors are caught and logged to console.\n",
    "\n",
    "    Example:\n",
    "        >>> get_similar_words_rag(\"apple\", \"\", k=3)\n",
    "        ['fruit', 'macintosh', 'granny smith']\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query retriever for documents related to the input word\n",
    "        results = retriever.invoke(word)\n",
    "\n",
    "        # Ensure results is always a list for consistent iteration\n",
    "        if not isinstance(results, list):\n",
    "            results = [results]\n",
    "\n",
    "        similar_words = []\n",
    "\n",
    "        # Process up to k retrieved documents\n",
    "        for doc in results[:k]:\n",
    "            try:\n",
    "                # Try to parse content as JSON and extract dictionary keys\n",
    "                data = json.loads(doc.page_content)\n",
    "                similar_words.extend(data.keys())\n",
    "            except json.JSONDecodeError:\n",
    "                # If parsing fails, treat content as plain text\n",
    "                similar_words.append(doc.page_content)\n",
    "\n",
    "        return similar_words\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the retrieval failure\n",
    "        print(f\"[DEBUG] RAG search failed for '{word}': {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:40.087458Z",
     "iopub.status.busy": "2025-08-06T21:46:40.087116Z",
     "iopub.status.idle": "2025-08-06T21:46:40.110251Z",
     "shell.execute_reply": "2025-08-06T21:46:40.109150Z",
     "shell.execute_reply.started": "2025-08-06T21:46:40.087426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    text: str  \n",
    "    src_language: str\n",
    "    tgt_language: str\n",
    "    words: Set[str] \n",
    "    translations: Dict[str, List[str]] \n",
    "    definitions: Dict[str, List[str]]  \n",
    "    examples: Dict[str, List[str]] \n",
    "    examples_number: Dict[str, int]\n",
    "    similar_words: Dict[str, List[str]]  \n",
    "    saved_to_json: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:40.112592Z",
     "iopub.status.busy": "2025-08-06T21:46:40.112207Z",
     "iopub.status.idle": "2025-08-06T21:46:40.131609Z",
     "shell.execute_reply": "2025-08-06T21:46:40.130586Z",
     "shell.execute_reply.started": "2025-08-06T21:46:40.112567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "        'text':text_en, \n",
    "        'src_language':'en',\n",
    "        'tgt_language':'ko',\n",
    "        'words':set(),\n",
    "        \"translations\": {'park': []},\n",
    "        \"definitions\": {'park': []},\n",
    "        \"examples\": {'park': []},\n",
    "        \"examples_number\": {'park': 1},\n",
    "        \"similar_words\": {'park': []},\n",
    "        \"saved_to_json\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:46:43.187531Z",
     "iopub.status.busy": "2025-08-06T21:46:43.187018Z",
     "iopub.status.idle": "2025-08-06T21:46:43.198814Z",
     "shell.execute_reply": "2025-08-06T21:46:43.196730Z",
     "shell.execute_reply.started": "2025-08-06T21:46:43.187498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_dict_to_json(\n",
    "    dictionary: Dict[Union[str, Tuple[str, str]], Any],\n",
    "    file_path: str = \"dictionary.json\"\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Save a dictionary to a JSON file.\n",
    "\n",
    "    This function serializes the provided dictionary into JSON format and saves it to a file. \n",
    "    If the dictionary contains tuple keys, they will be automatically converted to strings \n",
    "    because JSON does not support tuple keys.\n",
    "\n",
    "    Args:\n",
    "        dictionary (Dict[Union[str, Tuple[str, str]], Any]):\n",
    "            The dictionary to save. Keys can be strings or tuples of strings.\n",
    "        file_path (str, optional):\n",
    "            The path to the JSON file where the dictionary should be saved.\n",
    "            Defaults to \"translations.json\".\n",
    "\n",
    "    Returns:\n",
    "        bool:\n",
    "            True if the dictionary was saved successfully, False if an error occurred.\n",
    "\n",
    "    Notes:\n",
    "        - Uses UTF-8 encoding for proper Unicode support.\n",
    "        - Sets `ensure_ascii=False` to preserve non-ASCII characters in readable form.\n",
    "        - Uses `indent=4` for pretty printing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert tuple keys to strings to make JSON serializable\n",
    "        serializable_dict = {\n",
    "            str(k) if isinstance(k, tuple) else k: v\n",
    "            for k, v in dictionary.items()\n",
    "        }\n",
    "\n",
    "        # Write dictionary to JSON file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(serializable_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"[ERROR] Failed to serialize dictionary to JSON: {e}\")\n",
    "        return False\n",
    "    except OSError as e:\n",
    "        print(f\"[ERROR] Failed to write to file '{file_path}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T22:38:13.919666Z",
     "iopub.status.busy": "2025-08-06T22:38:13.918815Z",
     "iopub.status.idle": "2025-08-06T22:38:13.939100Z",
     "shell.execute_reply": "2025-08-06T22:38:13.937732Z",
     "shell.execute_reply.started": "2025-08-06T22:38:13.919635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def translate_words_node(state: \"State\") -> dict:\n",
    "    \"\"\"\n",
    "    Node: Translate words from source language to target language.\n",
    "\n",
    "    This function:\n",
    "    1. Calls `generate_translation()` to translate the input text.\n",
    "    2. Extracts the set of words from the translation keys.\n",
    "    3. Returns updated `words` and `translations` in the state.\n",
    "\n",
    "    Args:\n",
    "        state (State): Current state containing `text`, `src_language`, and `tgt_language`.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state keys:\n",
    "            - 'words': set of unique words found in translation\n",
    "            - 'translations': dict with translations\n",
    "    \"\"\"\n",
    "    translations = generate_translation(\n",
    "        text=state[\"text\"],\n",
    "        src_language=codes_language[state['src_language']],  # Convert to language code\n",
    "        tgt_language=codes_language[state['tgt_language']]\n",
    "    )\n",
    "\n",
    "    # Extract words from translation keys\n",
    "    words = set(translations.keys())\n",
    "\n",
    "    return {\n",
    "        'words': words,\n",
    "        'translations': translations\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_definitions_node(state: \"State\") -> dict:\n",
    "    \"\"\"\n",
    "    Node: Generate definitions for translated words.\n",
    "\n",
    "    This function:\n",
    "    1. Iterates over existing words in `definitions`.\n",
    "    2. Calls `generate_definitions()` for each word.\n",
    "    3. Extends the definitions list if new ones are found.\n",
    "\n",
    "    Args:\n",
    "        state (State): Current state containing `definitions` and `src_language`.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated 'definitions' key in state.\n",
    "    \"\"\"\n",
    "    definitions = state['definitions']\n",
    "\n",
    "    for word in list(definitions.keys()):  # Copy keys to avoid runtime mutation issues\n",
    "        def_dict = generate_definitions(\n",
    "            word,\n",
    "            codes_language[state['src_language']]\n",
    "        )\n",
    "\n",
    "        # Append new definitions if they exist\n",
    "        if def_dict.get('definition'):\n",
    "            definitions[word].extend(def_dict['definition'])\n",
    "\n",
    "    return {'definitions': definitions}\n",
    "\n",
    "\n",
    "def generate_examples_node(state: \"State\") -> dict:\n",
    "    \"\"\"\n",
    "    Node: Generate example sentences for each word.\n",
    "\n",
    "    This function:\n",
    "    1. Uses `examples_number` to determine how many examples per word.\n",
    "    2. Calls `generate_examples()` for each word.\n",
    "    3. Extends the examples list if new ones are found.\n",
    "\n",
    "    Args:\n",
    "        state (State): Current state containing `examples`, `examples_number`, `definitions`.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated 'examples' key in state.\n",
    "    \"\"\"\n",
    "    examples = state['examples']\n",
    "\n",
    "    for word in examples.keys():\n",
    "\n",
    "        ex_number = 1\n",
    "\n",
    "\n",
    "        if state['examples_number'].get(word):\n",
    "            ex_number = state['examples_number'][word]\n",
    "\n",
    "        ex_dict = generate_examples(\n",
    "            word,\n",
    "            codes_language[state['src_language']],\n",
    "            state['definitions'][word],\n",
    "            ex_number\n",
    "        )\n",
    "\n",
    "        # Append examples if found\n",
    "        if ex_dict.get('examples'):\n",
    "            examples[word].extend(ex_dict['examples'])\n",
    "\n",
    "    return {\n",
    "        'examples': examples\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_similar_node(state: \"State\") -> dict:\n",
    "    \"\"\"\n",
    "    Node: Find semantically similar words for each word using RAG.\n",
    "\n",
    "    This function:\n",
    "    1. Iterates through `similar_words` keys.\n",
    "    2. Calls `get_similar_words_rag()` to retrieve similar words from the retriever.\n",
    "    3. Updates the list of similar words.\n",
    "\n",
    "    Args:\n",
    "        state (State): Current state containing `similar_words` and `text`.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated 'similar_words' key in state.\n",
    "    \"\"\"\n",
    "    similar_words = state['similar_words']\n",
    "\n",
    "    for word in similar_words.keys():\n",
    "        synonyms = get_similar_words_rag(word, state['text'])\n",
    "        similar_words[word] = synonyms\n",
    "\n",
    "    return {'similar_words': similar_words}\n",
    "\n",
    "\n",
    "def save_to_json_node(state: \"State\") -> dict:\n",
    "    \"\"\"\n",
    "    Node: Save merged dictionary data to JSON.\n",
    "\n",
    "    This function:\n",
    "    1. Merges words, translations, definitions, and examples into a final dict.\n",
    "    2. Saves it to JSON using `save_to_json()`.\n",
    "    3. Stores 'final_dict' and 'saved_to_json' in the state.\n",
    "\n",
    "    Args:\n",
    "        state (State): Current state with 'words', 'translations', 'definitions', 'examples'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state keys:\n",
    "            - 'final_dict': full merged dictionary\n",
    "            - 'saved_to_json': result of saving operation (bool)\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "\n",
    "    for word in state[\"words\"]:\n",
    "        merged[word] = {\n",
    "            \"translations\": state[\"translations\"].get(word, []),\n",
    "            \"definitions\": state[\"definitions\"].get(word, []),\n",
    "            \"examples\": state[\"examples\"].get(word, [])\n",
    "        }\n",
    "    final_merged = {(state['src_language'], state['tgt_language']) : merged}\n",
    "    # Save merged dictionary\n",
    "    saved = save_dict_to_json(final_merged)\n",
    "\n",
    "    return {\n",
    "        'final_dict': final_merged,\n",
    "        'saved_to_json': saved\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T22:38:14.288581Z",
     "iopub.status.busy": "2025-08-06T22:38:14.288190Z",
     "iopub.status.idle": "2025-08-06T22:38:14.310858Z",
     "shell.execute_reply": "2025-08-06T22:38:14.309833Z",
     "shell.execute_reply.started": "2025-08-06T22:38:14.288554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"translate_words\", translate_words_node)\n",
    "graph.add_node(\"generate_definitions\", generate_definitions_node)\n",
    "graph.add_node(\"generate_examples\", generate_examples_node)\n",
    "graph.add_node(\"generate_similar\", generate_similar_node)\n",
    "graph.add_node(\"save_to_json\", save_to_json_node)\n",
    "\n",
    "graph.add_edge(START, \"translate_words\")\n",
    "graph.add_edge(\"translate_words\", \"generate_definitions\")\n",
    "graph.add_edge(\"generate_definitions\", \"generate_examples\")\n",
    "graph.add_edge(\"generate_examples\", \"generate_similar\")\n",
    "graph.add_edge(\"generate_similar\", \"save_to_json\")\n",
    "graph.add_edge(\"save_to_json\", END)\n",
    "\n",
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T22:38:17.002457Z",
     "iopub.status.busy": "2025-08-06T22:38:17.002061Z",
     "iopub.status.idle": "2025-08-06T22:38:17.127109Z",
     "shell.execute_reply": "2025-08-06T22:38:17.126247Z",
     "shell.execute_reply.started": "2025-08-06T22:38:17.002424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image , display\n",
    "\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T22:38:17.169652Z",
     "iopub.status.busy": "2025-08-06T22:38:17.169275Z",
     "iopub.status.idle": "2025-08-06T22:38:17.176037Z",
     "shell.execute_reply": "2025-08-06T22:38:17.174897Z",
     "shell.execute_reply.started": "2025-08-06T22:38:17.169608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_state = {\n",
    "        'text':text_en, \n",
    "        'src_language':'en',\n",
    "        'tgt_language':'ko',\n",
    "        'words':set(text_en.split()),\n",
    "        \"translations\": {'park': []},\n",
    "        \"definitions\": {'park': []},\n",
    "        \"examples\": {'park': []},\n",
    "        \"examples_number\": {'park': 1},\n",
    "        \"similar_words\": {'park': []},\n",
    "        \"saved_to_json\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T22:38:17.333616Z",
     "iopub.status.busy": "2025-08-06T22:38:17.333104Z",
     "iopub.status.idle": "2025-08-06T22:42:41.833021Z",
     "shell.execute_reply": "2025-08-06T22:42:41.831945Z",
     "shell.execute_reply.started": "2025-08-06T22:38:17.333589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = compiled_graph.invoke(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
